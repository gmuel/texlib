\section{Differential Rings and Modules}
Recalling our definition of differential rings (an associative algebra with derivation Lie algebra) we wish to extend our set of notations.
\subsection{Differential modules and ideals}
\begin{defi}
Let $(R,D)$ be a differential ring.
\bn
\item A differential module $(M,D_M)$ is an $R$-module with an additive map:
$$D : M \longrightarrow M,\ r m \longmapsto \partial_R(r) m + r D(m)\ \forall D \in D_M,\ r \in R,\ m \in M,$$
where $\partial_R \in \trm{Der}(R) = \left<D\right>$.
\item A differential ideal $I \subset R$ is a differential $R$-submodule of $R$, i.e. $\partial(I) \subset I$ for all $\partial \in D$.
\item Let $(R,D_R)$ and $(S,D_S)$ be two differential rings. A ring homomorphism $f : R \longrightarrow S$ is called differential if
$$\partial_S \circ f = f \circ \partial_R$$
or equivalently the following diagram commutes:
$$\xymatrix{
R \ar[d]_{\partial_R} \ar[r]^f & S\ar[d]^{\partial_S}\\
R \ar[r]_{f\mid_{\partial_R(R)}} & S.\\
}$$
%where $\partial_X(X) := \trm{im} \partial_X$ for $X = R, S$.
\item A differential ring $(R,D)$ is called simple differential if it has no proper differential ideals.
\item A differential field is a differential ring with no proper ideals (differential or non-differential).
\item Let $(R,D)$ be a differential ring. The subset
$$R^\partial := \{x \in R : \partial(x) = 0\ \forall \partial \in D\}$$
defines a subring and is called the ring of constants.
\en
\index{Index}{module!differential}
\index{Index}{ring!differential}
\index{Index}{ring!differential!homomorphism of}
\index{Index}{ring!differential!simple}
\index{Index}{ring!differential! of constants}
\index{Index}{field!of constants}
\index{Symbol}{$\trm{Der}_R(A)$}
\index{Symbol}{$R^\partial$}
\end{defi}
\bmk As mentioned, the derivation maps $D$ on differential modules $(M,D_M)$ are only additive (are in general not in $\trm{End}_R(M)$, but in $\trm{End}_{R^\partial}(M)$ - i.e. its ring of constants). This is due to the Leibniz-rule as defined above.\\
\indent The ring of constants $R^\partial$ for a differential field $(R, D)$ is also a field, since:
$$\partial(a a^{-1}) = 0 = \underbrace{\partial(a)}_{=0} a^{-1} + a \partial(a^{-1}) \LRA a \partial(a^{-1}) = 0$$
holds for all $a \in R^\times$.
\bsp Some prominent examples:
\bn
\item any ring $R$ is a (trivial) differential ring, via $0 : R \longrightarrow 0$. Thus, all $R$-modules are also differential modules via the zero-homomorphism.
\item The polynomial ring in one indeterminate:
$$\left(k[X], \partial = \frac{d}{dX}\right),$$
$k$ a field with characteristic zero, is a simple differential ring since all ideals $I$ generated by some polynomial of degree greater zero eventually fulfill $\partial^i(I) = (1)$. On the other hand, $(k[X], \partial = X \frac{d}{dX})$ has non-trivial differential ideals:
$$I_i = \left<X^i\right>,\ \forall i \geq 1$$
as each $k$-sub vector space $k.X^i$ is $\partial$-stable.
\item Let $p \in \nz$ be a prime number, the polynomial ring $\mathbb{F}_p[X]$ with $\mathbb{F}_p$-derivation $\partial = \frac{d}{d X}$, as in the last example, has an interesting property. Its ring of constants is $\mathbb{F}_p[X^p]$ as $\partial(X^p) = p X^{p-1} = 0$ and this ring contains indeed non-trivial differential ideals (in contrast to characteristic zero fields):
$$I_k := \left<X^{p^k}\right>,\ k \geq 1$$% : k \geq 1\right>,$$
since $\partial(X^{p^k} f + X^{p^k} g) = \partial(X^{p^k} f) + \partial(X^{p^k} g) = X^{p^k} \partial(f + g)$ for all $f, g \in \mathbb{F}_p[X]$.
\item The field of rational functions $k(X)$ is a differential field with derivation Lie algebra generated by
$$D = \left\{\partial'_x = \left[\frac{f}{g} \mapsto \frac{\partial_x(f) g - f \partial_x(g)}{g^2}\right] : x \in X\right\}.$$
\item Let $U \subset \rz^n$ be open and connected then $(C^\infty(U),D=\{\partial_i : 1 \leq i \leq n\})$ is a partial differential ring with a non-trivial differential ideal $\mathfrak{m} := \{f \in C^\infty(U) : \trm{supp} (f) \subsetneq U\}$, as we may define a proper differential ideal for all $f \in C^\infty(U) \bsl \rz[\{x\}]$ by simply putting
$$\left<\{f\}\right> = \left<\partial_{i_1} \circ \hdots \circ \partial_{i_k}(f) : 1\leq i_j \leq n,\ k \in \nz_0\right>,$$
where $\rz[\{x\}]$ denotes the ring of real convergent power series in $U$. These ideals are, in general, not finitely generated.
\en
\subsection{General Differential Algebra}
Let $(k,D)$ be a differential ring with $D = \{\partial\}$ and $k^\partial$ its ring of constants.
\subsubsection{Ring of differential operators}
In terms of Ore extension, the subring $D := k[\partial] \subset \trm{End}_{k^\partial}(k)$ is isomorphic to the Ore-extension $k[X,id_k,\partial]$ over $k$. To show this we depend on its intrinsic module algebra structure:\\
$$\Psi := \Psi_{\trm{int}} \mid_{D \otimes k.id_D} = \left[d' \longmapsto \left[d \longmapsto \mu_D(d' \otimes d) = d'(d)\right]\right],$$
where $\Psi_{\trm{int}} : D \otimes \trm{End}_{k^\partial}(D) \longrightarrow \trm{End}_{k^\partial}(D)$, restricted to the subalgebra $k.id_D$.
\indent $k[\partial]$ has the structure of a pointed-irreducible bialgebra, of Birkhoff-Witt type (only $1_{k[\partial]}$ as group-like, generator $X \in \mathfrak{g}_{k^\partial}(k)$ has primitive coproduct and $(k[\partial],\mu,\eta)$ is isomorphic to the enveloping algebra of some Lie-algebra). Its structure map
$$\Psi : D \otimes D \longrightarrow D,\ \partial \otimes a \partial^i \longmapsto \partial(a) \partial^i + a \partial^{i+1}.$$
To see the isomorphism wrt. Ore-extensions, we consider the $k$-left module $k.\partial^i$ with its above described $k$-right module structure:
$$\partial^i \otimes a \sim \sum_{j=0}^i \left(\bao{c}i\\j\\\ea\right) \partial^j(a) \otimes \partial^{i-j}$$
Hence, the ideal generators $1 \otimes X \otimes a - a \otimes X \otimes 1 - \partial(a) \in \bigoplus_{n \geq 0} (k \otimes \partial)^{\otimes n} \otimes k$, as in \ref{prop03} on pg. \pageref{prop03}, yield - evaluated via $id_k \otimes \Psi : k \otimes D \otimes k \longrightarrow k \otimes k \simeq k$:
$$id \otimes \Psi(1 \otimes X \otimes a - a \otimes X \otimes 1 - \partial(a) \otimes 1_D \otimes 1_k) = X(a) - a X(1) - \partial(a).$$
Identifying $X = \partial$, we get indeed zero as desired. Hence, the quotient yields our desired Ore-extension $k[X,id_k,\partial] \simeq k[\partial]$. There are more interesting properties for $k[\partial]$ that are discussed in \cite{vdPS01}, chapter 2, in greater detail.\\
\indent Concluding this subsection, we note that $k[\partial]$ is a unital, associative, coassociative, cocommutative bialgebra (indeed, has an antipode, which will be discussed below) acting on $k$ via evaluation.
\subsubsection{Ring of differential polynomials}\label{RingOfDiffPolys}
As above we are using a differential field $(k,\partial)$, $k^\partial$ its ring of constants and its ring of differential operators $k[\partial]$ being generated by one element $\partial$.%, pick some differential field $(k,\partial)$ and
 We consider the ring of polynomials $R := k[u_1,\ldots,u_n]$, i.e. a noetherian ring over $k$. In general, we have no unique extension of $\partial$ to $R$ except for that of a trivial differential ring: $u_i \longmapsto 0$ (enlarging the ring of constants to $R^\partial[u_1,\ldots,u_n]$). However, we may use a non-noetherian transcendental extension $S$ over $R$
$$S := k[u_{i,j} : 1 \leq i \leq n, j \in \nz_0],$$ where $u_{i} \longmapsto u_{i,0}$ defines an embedding of $R$ in $S$. Indeed, this ring still has no canonical $D = k[\partial]$-module algebra structure. However, the module:
$$D \otimes_k S := D \otimes_{k^\partial} S/\left<d \otimes x s - \sum_{(d)} d_{(1)}(x)d_{(2)} \otimes s : x \in k, d \in D, s \in S\right>,$$
where $\Delta(d) = \sum_{(d)} d_{(1)} \otimes d_{(2)}$, has:
$$\partial^i \otimes f_\alpha u^\alpha \sim_\partial \sum_{j=0}^i\left(\bao{c}i\\j\\\ea\right) \partial^j(f_\alpha) \partial^{i-j}\otimes u^\alpha.$$
Its module algebra structure map $\Psi : D \otimes_k (D \otimes_k S) \longrightarrow D \otimes_k S$ is given by the above equivalence relation. The ideal
$$I := \left<\partial^{\alpha_3} \otimes u_{\alpha_1,\alpha_2} - \partial^\beta \otimes u_{\alpha_1,\alpha_2 + \alpha_3 - \beta} : 1 \leq \alpha_1 \leq n, \alpha_2, \alpha_3 \in \nz_0, 0 \leq \beta \leq \alpha_3 - 1\right> \subset D \otimes_k S$$ gives us our desired:
\begin{defi}[Ring of differential polynomials]
Let $S$ and $I$ be defined as above. The quotient ring $D \otimes_k S/I$ is called the ring of differential polynomials and is denoted by:
$$k\left\{u_1,\ldots,u_n\right\} := D \otimes_k S/I.$$
\end{defi}
\bmk %Firstly, we remark that if $\partial u_{i,j} = u_{i,j+1} \in K\{u\}$ then also 
%$$\partial^l(u_{i,j}) = \partial^{l-1}(u_{i,j+1}) = \ldots = u_{i,j+l} \in k\{u\}\ \forall 1 \leq i \leq n,\ j \geq 0,\ l \geq 1.$$
Sometimes we may use $u_i^{(j)}$ instead of $u_{i,j}$. Secondly, note that this, indeed, defines a (non-noetherian) differential ring, with derivations $\{\partial, \partial_{u_{i,j}}\}$. Here, $\partial_{u_{i,j}}$ are $k$-derivations, while $\partial$ is a $k^\partial$-derivation. Additionally, we may still recover $R$:
$$R \simeq k\{u_1,\ldots,u_n\}/J,\ \trm{where}\ J := \left<u_{i,1} - 1, u_{i,j} : 1 \leq i \leq n, j \geq 2\right> \subset k\{u_{i,j}\}.$$
\indent The definition naturally translates to differential rings - if $(R,\partial)$ is our differential ring, with ring of constants $R^\partial$ and $D$-stable ideal $I$ as above, then
$$R\{u_1,\ldots,u_n\} := D \otimes_k R[u_{i,j} : 1 \leq i \leq n, j \geq 0]/I.$$
Although being non-noetherian (any non-trivial differential ideal does not fulfill the ascending chain condition), the factor rings we will consider are in fact noetherian.
\begin{defi}
Let $(k,\partial)$ be a differential ring (field). An associative unital $k$-algebra $K$ is called a differential extension over $k$, short $K/k$, if there is an $n \in \nz$ and some differential ideal $I \subset k\{u_1\ldots u_n\}$ such that
$$K \simeq k\{u_1\ldots u_n\}/I.$$
\index{Index}{extension!differential}
\end{defi}
\bmk Given a differential ring $(k,\partial)$ and a finite family of differential polynomials $\mathcal{F} \subset k\{u_1\ldots u_{|\mathcal{F}|}\}$ the associated differential ideal $I$ is simply the differential saturation:
$$I := \left<\partial^i(f) : f \in \mathcal{F},\ i \geq 0\right>.$$
Such a family is called a differential equation - linear if the degree of all monomials is at most one, otherwise non-linear. We call it an explicit differential equation if each element in $\mathcal{F}$ is linear with respect to $\partial u_i$ and all coefficients of monomials of the form $\prod_{i,j} \partial^j u_i$ are zero, for $j \geq 2$. Otherwise, it is called implicit.
\index{Index}{differential saturation}
\index{Index}{differential equation}
\index{Index}{differential equation!linear}
\index{Index}{differential equation!non-linear}
\index{Index}{differential equation!explicit}
\index{Index}{differential equation!implicit}
\begin{defi}
Let $k\{u_1,\hdots,u_n\} =: k\{u\}$ be the ring of differential polynomials over some differential field $k$, $k[\partial]$ the left $k$-module of differential operators on $k\{u\}$. The map
$$\bao{rrcl}
ev &: k[\partial] \otimes_{k^\partial} k\{u\} & \longrightarrow &k\{u\}\\
&&&\\
& \sum_{\substack{0 \leq i \leq n\\\alpha \in \nz_0^k\\k\geq 0}} (a_i \partial^i \otimes b_\alpha u_\alpha) &\longmapsto& \sum_{\substack{i,\alpha\\k \leq i}} \left(\bao{c}i\\k\\\ea\right) a_i \partial^k(b_\alpha) \partial^{i-k}(u_\alpha)\\
\ea$$
is called the evaluation homomorphism.
\end{defi}
\bmk %The evaluation homomorphism gives us a $k[\partial]$-module algebra structure on $k\{u\}$, where $(k[\partial],\mu,\eta,\Delta,\eps)$ is the bialgebra structure on $k[\partial]$ (infact, it has a Hopf-algebra structure via $S : k[\partial] \longrightarrow k[\partial]$, $\partial \longmapsto -\partial$.
Obviously, the definition $ev$ and $\Psi$ are equal. Using this setting we get
\begin{koro}
The two-sided $k$-module $k[\partial]$ defines (in general non-commutative if $k^\partial \neq k$) a unital $k$-algebra.
\end{koro}
\bws As $k$ is a unital commutative simple algebra over $k^\partial$ and $k[\partial]$ is a Ore-extension as shown above, there is nothing more to show.%Note that the map $\mu : k[\partial] \otimes k[\partial] \longrightarrow k[\partial], a \partial^i \otimes b \partial^j \longmapsto \sum \left(\bao{c}i\\k\\\ea\right) a \partial^k(b) \partial^{i+j-k}$ defines a multiplication on the monomial terms of $k[\partial]$. The unit is simply $\eta : K \longrightarrow K[\partial], 1_k \longmapsto 1_{k[\partial]}$.
\bmk Moreover, $k[\partial]$ has a $k^\partial$-coalgebra structure:
$$\Delta =\left[ \partial^i \longmapsto \sum \left(\bao{c}i\\j\\\ea\right) \partial^{i-j} \otimes \partial^j\right],\ \eps = [\partial^i \longmapsto \delta_{i,0}]$$
making it to a $k^\partial$-bialgebra as clearly: $\eps \eta = id_k$, $(\eta \otimes \eta) \circ \eta = \Delta \eta = id_{k.1_{k[\partial]} \otimes k.1_{k[\partial]}}$.% With $S := [\partial^i \longmapsto (-1) \partial^i]$ we get an antipode since $\mu((S\otimes id)(1_k)) = \mu((id_k \otimes S)(1_k)) = \eta(\eps(1_k))$.
\begin{prop}
The following statements are equivalent:
\bn
\item\label{item01} $k\{u\}$ is a $k[\partial]$-module algebra (or $\Psi := ev$ defines a module algebra structure on $k\{u\}$).
\item \label{item02} Given the evaluation homomorphism and multiplication on $k[\partial]$ then the following diagram commutes
$$\xymatrix{
k[\partial] \otimes k[\partial] \otimes k\{u\} \ar[r]^{\mu \otimes id_{k\{u\}}}\ar[d]_{id_{k[\partial]} \otimes ev} & k[\partial] \otimes k\{u\}\ar[d]^{ev}\\
k[\partial] \otimes k\{u\} \ar[r]_{ev} & k\{u\}\\
}$$
\en
\end{prop}
\bws We show first, that the second statement is indeed true.
\bn
\item By simple computation on the monomial terms $a \partial^i, b \partial^j \in k[\partial]$ and $c u_\alpha \in k\{u\}$ we get
{\scriptsize
$$\bao{rcl}
ev(\mu \otimes id_{k\{u\}})(a \partial^i \otimes b \partial^j \otimes c u_\alpha) &=& \sum_{k' \leq i} \sum_{l' \leq i + j - k'} \left(\bao{c}i\\k'\ea\right) \left(\bao{c}j + i - k'\\l'\ea\right) a \partial^{k'}(b) \partial^{l'}(c) \partial^{j+i-k'-l'}(u_\alpha)\\
&&\\
ev(id_{k[\partial]} \otimes ev)(a \partial^i \otimes b \partial^j \otimes c u_\alpha) &=& \sum_{k \leq i} \sum_{l \leq j} \sum_{m \leq i - k} \left(\bao{c}i\\k\ea\right) \left(\bao{c}j\\l\ea\right) \left(\bao{c}i - k\\m\ea\right) a \partial^k(b) \partial^{l+m}(c) \partial^{j+i-k-l-m}(u_\alpha)\\
\ea$$}
Fixing $l' \leq j$ and putting $k = k'$ we see that our equivalence implies $$\sum_{l + m = l'} \left(\bao{c}j\\l\ea\right) \left(\bao{c}i - k\\m\ea\right) a \partial^k(b) \partial^{l+m}(c) \partial^{j+i-k-l-m}(u_\alpha) = \left(\bao{c}j + i - k\\l + m\\\ea\right) a \partial^k(b) \partial^{l + m}(c) \partial^{j + i - l - m - k}(u_\alpha).$$
Since the degree of the differential operators on each factor do agree, we get
$$\sum_{l + m = l'} \left(\bao{c}j\\l\ea\right) \left(\bao{c}i - k\\m\ea\right) = \left(\bao{c}j + i - k\\l + m\\\ea\right).$$
But this is just a rewriting of the Vandermonde identity for all $k \leq i$ and proves our claim.
\item If $k\{u\}$ is a $k[\partial]$ module algebra we have that $(x y) v = x(y v)$ for all $x, y \in k[\partial]$, $v \in k\{u\}$. Expanding with our standard notation this translates into $ev(\mu\otimes id) = ev(id\otimes ev)$ proving \ref{item01} $\RA$ \ref{item02}.\\
The opposite direction follows immediately from the definition.
\en
\begin{koro}\label{HopfModAlgDiffPoly}
With bialgebra structure maps $\mu, \eta, % = [1_k \longmapsto 1_{k[\partial]}], 
\Delta$ and $ % = \left[a \partial^i \longmapsto \sum_{0 \leq j \leq i} \left(\bao{c}i\\j\\\ea\right) a \partial^j \otimes \partial^{i - j}\right], 
\eps$ as defined above and the $k$-homomorphism
% = [1_{k[\partial]} \longmapsto 1_k, a \partial^i \longmapsto 0],
$$S : k[\partial] \longrightarrow k[\partial],\ \partial^i \longmapsto (-1)^i \partial^i,$$
we have that $k\{u\}$ is a $k[\partial]$-Hopf-module algebra.
\end{koro}
\bws Note that the generator $\partial$ is a primitive cocommutative Hopf algebra element, i.e. $\Delta(\partial) = 1 \otimes \partial + \partial \otimes 1$ implying $S(\partial) = - \partial$. We simply have to show that the commutative diagram for Hopf-algebras does commute.
$$\partial^i \stackrel{\Delta}{\longmapsto} \sum_{j=0}^i\left(\bao{c}
i\\
j\\
\ea\right) \partial^j \otimes \partial^{i-j} \stackrel{S \otimes id}{\longmapsto} \sum_{j} \left(\bao{c}
i\\
j\\
\ea\right) (-1)^j \partial^j \otimes \partial^{i-j} \stackrel{\mu}{\longmapsto} \sum_j \left(\bao{c}
i\\
j\\
\ea\right) (-1)^j \partial^i.$$
According to Pascals rule we see via induction that except for $i = 0$ all sums are zero. As $\eta(\eps(\partial^i)) = \delta_{i,0}$, we have just shown the required commutativity.
%Hence, all elements $\sum c_\alpha u_\alpha$ can be associated with some element of the form $\sum \left(\bao{c} i\\j\\\ea\right) \partial^j(b_\alpha) \partial^{i-j}(u_\alpha)$ we get an (not necessarily unique) element such that for $\rho = \left[\sum c_\alpha u_\alpha \longmapsto \sum \left(\bao{c}i\\j\\\ea\right) \partial^j(b_\alpha) \otimes \partial^{i-j}(u_\alpha)\right]$
\subsection{Linear Differential Equations}
Let $(k,\partial)$ be a (not necessarily non-trivial with characteristic zero) differential field with field of constants denoted by $k^\partial$ and $M$ a noetherian differential module over $k$ (i.e. finite dimensional vector space, with derivation).
\begin{defi}
A (scalar) linear differential equation is $k^\partial$-linear map $L : M \longrightarrow M$, with $L = \sum a_i \partial^i$, i.e. $L \in k[\partial] \subset \trm{End}_{k^\partial}(k \otimes_{k^\partial} M)$. The solution space is the $k^\partial$-subspace of $M$:
$$S(L) := \{x \in M : L(x) = 0\}.$$
\index{Index}{space!solution}
\index{Symbol}{$S(L)$}
\end{defi}
Alternatively, one can define linear differential equations simply via linear algebra:
$$\partial(x) = A x,\ A \in \trm{End}(M),\ x \in M.$$
Its solution space is simply generated by the kernel elements of $\partial - A$ in $M$. Hence, all differential extensions are generated by solutions over $k$. Now, an important definition:
\begin{defi}
Let $K/k$ be a differential extension to the differential equation $\partial(x) = Ax$.
\bn
\item we call a matrix $X \in \trm{Mat}_n(K)$ a solution matrix, if
$$\partial(X) = A X \in \trm{Mat}_n(K).$$
If $X = (x_{ij}) \in \trm{Mat}_n(K)$ is a solution matrix for the above differential equation we call the $n^2 \times n^2$ matrix
$$Wr(X) = \left(\partial^l(x_{ij})\right)_{\substack{0 \leq l \leq n^2-1\\1 \leq i,j \leq n}}$$
the Wronskian matrix. Its determinant is simply called Wronskian.
\item $K/k$ is called a differential extension (over $k$), if $K$ contains the solution space for some linear differential equation $\partial(x) = A x$.
\en
%A solution matrix in the general linear group $\trm{Gl}_n(R)$ is called a fundamental matrix.
\index{Index}{matrix!solution}
\index{Index}{matrix!Wronskian}
\index{Index}{matrix!fundamental}
\index{Index}{Wronskian}
\end{defi}
\bmk As we already defined differential extensions via the ring of differential polynomials $k\{u\}$ (to be precise via differential quotient rings) we shall show both definitions are equivalent. But clearly, the family of differential polynomials is simply $\mathcal{F} := \{\partial u_i - \sum_j a_{i,j} u_j \in k\{u\} : 1 \leq i \leq n\}$ for a given differential equation $\partial u = A u$.\\
\indent Secondly, we get the Wronskian matrix by constructing column vectors $$y_{ij} = \left(x_{ij},\partial(x_{ij}),\ldots,\partial^{n^2-1}(x_{ij})\right)^t$$running over all indices $1 \leq i,j \leq n$. Furthermore, the definition of the Wronskian is broader - for some differential extension $R/k$ and elements
$y_1, \ldots, y_m \in R$ the Wronskian matrix is simply $Wr(y_1,\ldots,y_m) := (\partial^l(y_i))_{1 \leq i,l + 1 \leq m}$.
\begin{defi}
Let $K/k$ be a differential field extension for a given differential equation $\partial y = A y$.
\bn
\item A Picard-Vessiot ring $R$ is a sub-ring of $K$, such that
$R^\partial = k^\partial$, $R$ contains no non-trivial differential ideals and there exists a solution matrix $X \in \trm{Gl}_n(R)$.
\item A solution matrix in a PV-ring $R$ is called a fundamental matrix.
\item A Picard-Vessiot field is the localization of a Picard-Vessiot ring.
\en
\end{defi}
\index{Index}{extension!differential}
\index{Index}{extension!Picard-Vessiot}
\index{Index}{ring!Picard-Vessiot}
\index{Index}{field!Picard-Vessiot}
The last definition requires a little
\begin{lemm}
A simple differential ring is zero-divisor free.
\end{lemm}
\bws Let $a \in \trm{Ann}(R)$, then there is a $b \in \trm{Ann}(R)\bsl\{0\}$, s.t. $a b = 0$. We get $\partial(a b) = 0 = \partial(a) b + a \partial(b) \LRA \partial(a) b = -a \partial(b)$. Multiplying both sides with $b$ we have:
$$\partial(a) b^2 = -a b \partial(b) = 0,$$
i.e. $\partial(a) \in \trm{Ann}(R)$. As the only proper differential ideal is zero, we see that $\trm{Ann}(R)$ is trivial.\\
Now, we have that indeed the localization of a Picard-Vessiot ring is well-defined in that sense that the localization is not the zero ring. Alternatively, we could define the Picard-Vessiot field extension $K/k$ as a field containing the solution space and having the same field of constants, i.e. $K^{\partial} = k^{\partial}$.
\newcommand{\minpoly}[1]{\trm{Min}(\alpha,#1)}
\newcommand{\minpolyC}{\minpoly{k^\partial}}
\newcommand{\minpolyR}{\minpoly{R}}
\begin{lemm}
Let $(k,\partial)$ be a differential field of characteristic zero and let $R$ be a differential subring of $k$
with the same field of constants, i.e. $R^\partial = k^\partial \subset R \subset k$.
\bn
\item If $\alpha \in R$ is algebraic over $k^\partial$, i.e. $\minpolyC \in k^\partial[X]$, with $k^\partial(\alpha) \simeq k^\partial[X]/\left<\minpolyC\right>$, then $\partial(\alpha) = 0$.
\item If $\alpha \in k$ is algebraic over $R$ and $\partial(\alpha) = 0$, then $\alpha$ is algebraic over $k^\partial$.
\en
\end{lemm}
\bws Let $\minpoly{S} = p = \sum_{i=0}^n p_i X^i \in S[X]$ for $S = R, k^\partial$.
\bn
\item By definition, we have $p \in k^\partial[X]$. Hence, evaluating $p$ at $\alpha$ in $R$ gives
$$p(\alpha) = \sum_{i=0}^n p_i \alpha^i = 0$$
Differentiating:
$$\partial(p(\alpha)) = \sum_{i=1}^n p_i \partial(\alpha^i) = \sum_{i=1}^n i p_i \partial(\alpha) \alpha^{i-1} = \left(\sum_{i=0}^{n-1} (i + 1) p_{i+1} \alpha^i\right) \partial(\alpha) = 0$$
Since $p_n = 1$ we see that the left hand factor cannot be zero. On the other hand, $k^\partial(\alpha)$ is an integral domain. Hence, $\partial(\alpha) = 0$.
\item We interpret 'algebraic over $k$ as 'integral over $R$. Thus, there is a monic polynomial as defined above over $R$. Proceeding as in the last part (i.e. evaluating $p$ at $\alpha$ in $R[\alpha]$ and differentiating), we get
$$\partial(p(\alpha)) = \sum_{i=0}^{n-1} \left(\partial(p_{i}) + (i + 1) p_{i+1} \partial(\alpha)\right) \alpha^i + \partial(p_n) \alpha^n = 0.$$
Since $p_n = 1$ we get
$$\partial(p_i) + \underbrace{(i + 1) p_{i+1} \partial(\alpha)}_{=0} = 0$$
for all $i = 0,\cdots,n - 1$. As $\trm{char} R = 0$ implies $\partial(p_i) = 0$ showing $p \in k^\partial[X]$.
\en

\begin{prop}\label{PicardVessiotRing}
Let $\partial(x) = A x$, as above.
\bn
\item A Picard-Vessiot ring $R$ is isomorphic to
$$k[x_{ij},1/\det X],$$
where $X = (x_{ij}) \in \trm{Gl}_k(M)$ is a fundamental matrix.
\item A matrix $X \in \trm{Mat}_n(R)$ is a fundamental matrix, if and only if its Wronskian is non-zero over $k^\partial$.
%\item The entries of the largest sub matrix of the Wronskian matrix  $Wr(L) = \left(x^{(k)}_{ij}\right)_{\substack{0 \leq k \leq n^2 - 1\\1 \leq i, j \leq n}}$ is a $C$-basis of the solution space, if and only if its Wronskian matrix has non-zero determinant. The associated fundamental matrix $X$ is spanned by this basis.
\item \label{PVLemma3}Two fundamental matrices $X_1, X_2$ are right-associated wrt. $\trm{Gl}_n(k^\partial)$.
\item \label{PVLemma4}Two PV-rings $R_1, R_2$ of the same equation are isomorphic as differential rings.
\en
\end{prop}
\bws A proof can be found in \cite{vdPS01}, pg. 15. However, for the last two statements we are going to present a sketch of proof:
\bd
\item[ad \ref{PVLemma3}] We have: $\partial(X_{1,2}) = A X_{1,2}$ and assume: $X_2 = X_1 M$ for some $M \in \trm{Gl}_n(R)$. Then:
$$\partial(X_2) = \partial(X_1 M) = \partial(X_1) M + X_1\partial(M) = A X_1 M + X_1 \partial(M) \stackrel{!}{=} A X_2 \LRA M \in \trm{Gl}_n(k^\partial).$$
We recall that $X_i \in \trm{Gl}_n(R)$ and the $X_1, X_2$ are left-associated if and only if $[M, A] = 0$.
\item[ad \ref{PVLemma4}] Let $S(L)_i$ denote the two solution spaces. As we just saw, we may define an isomorphism of differential modules $\phi : S(L)_1 \longrightarrow S(L)_2, x_{ik} \longmapsto \sum_{j} x_{ij} m_{jk}$. As the PV-ring is commutative, we deem $\trm{Sym}(S(L)_i)$ as an appropriate choice for construction (not necessarily the PV-ring, rather a subring). Now, we may extend $\phi$ to both algebras:
$$s = \sum s_i x^i = \sum s_i x_1^{i_1} \ldots x_{n}^{i_n} \longmapsto \sum s_i \phi(x_1)^{i_1} \ldots \phi(x_n)^{i_n}.$$
Since $1_{S(L)_1} \longmapsto 1_{S(L)_2}$ and $\phi$ is an isomorphism on $\trm{Sym}^1(S(L)_i)$ we get the desired isomorphism of differential rings. Localizing both algebras wrt. $\det X_i$ we have that $R_i \simeq S_{\det X_i} (L)_i$ for $i = 1,2$.
\ed
\bmk $\trm{Sym}(S(L)) \simeq k[x_1,\ldots,x_n]$ is not a simple differential ring (the maximal ideal $I = \left<x_i : 1 \leq i \leq n\right>$ is closed under $\partial$-action). Nevertheless, its localization wrt. $\det X = \sum_{\sigma \in S_n} sign(\sigma) \prod_{i=1}^n x_{i,\sigma(i)}$ is a simple differential ring, as $S_{\det X}^{-1}I$ contains units ($S_{\det X}^{-1}I = R$).\\
\indent In \cite{vdPS01}, the following theorem is given (prop. 2.9, pg. 40 and lem 2.10, pg. 41):
\begin{satz}
Let $\partial(x) = A x$ be a linear differential equation. There are $L_i \in k[\partial]$, such that
$$V_A := \{y \in M : \partial(y) = A y\} \simeq \bigoplus_{i=1}^n k[\partial]/k[\partial].L_i^*.$$
To specify, each matrix equation has a solution space $V_A$ isomorphic to the solution space of a scalar equation $L = \prod L_i$.
\end{satz}
The decomposition is not unique wrt. left- and right-sidedness, as $k[\partial]$ is in general not commutative. We are not going to prove this theoremo - only loosely scatching a proof.\\
First we note, for any field $k$ and a matrix $A \in \trm{Mat}_n(k)$, there exists vectors $c_i \in \bigoplus_{i=0}^n k.e_i$ such that $B_i = \left\{c_i,...,A^{j_i} c_i : i\right\}$ is a $k$-basis. The elements $c_i$ are called cyclic vectors, each subspace $M_i$ generated by such an element is called a cyclic vector space and the transformed matrix wrt. to the cyclic basis is called the rational normal form of $A$. If $\chi_A \in k[X]$ is the charactistic polynomial of $A$ and $\prod_{j = 1} p_j^{s_j}$ a prime decomposition of $\chi_A$ than its minimal polynomial, which is the monic generator of $\{f \in k[X] : f(A) = 0\}$, gives us all distinct cyclic vectors (each cyclic vectors space has different dimension $t_j \deg p_j$, where $p_j^{t_j} \mid m_A$ and $p_j^{t_j+1} \nmid m_A$). A similar reasoning is provided in \cite{vdPS01}, chapter 2. Here, a cyclic decomposition is given for any linear homogeneous differential equation.
% due to the maximality of $I$: any differentially closed ideal containing $I$ is already $R = (1)$. To see that its ring of constants $R^\partial = C$, consider $f = \sum_{i} f_i x_1^{i_1} \ldots x_n^{i_n}/\det X^{i_{n+1}} \in \ker \partial_R$:
%$$\partial(f) := \sum_i \partial(f_i) x^i/\det X^{i_{n+1}} + \sum_i i_k f_i x_1^{i_1} \ldots \partial(x_k) x_k^{i_k-1} \ldots x_n^{i_n}/\det X^{i_{n+1}}$$
\subsubsection{Differential Galois group}
In the theory of field extensions, the Galois group is the set of $k$-vector space automorphisms permuting the root elements of a given polynomial over $k$ leaving $k$ fixed. For a given linear differential equation, the definition is slightly different:
\begin{defi}
Let $\partial(x) = A x$ for some $A \in \trm{Gl}_k(M)$ and $x \in M$. The differential Galois group is the zentralizer of $\left<\partial\right> \subset \trm{Gl}_R(M)$, i.e.
$$\trm{DGal}(R/k) = \{\varphi \in \trm{Gl}_{k^\partial}(M) : \partial \varphi = \varphi \partial\}.$$
\index{Symbol}{$\trm{DGal}(R/k)$}
\end{defi}
This property is also called equivariance (e.g. in algebraic geometry).
%\paragraph{Examples for the linear case}
\begin{satz}[Galois correspondence]
Let $K/k$ be a PV extension for $\partial(x) = A x$. Let $\mathfrak{G}$ be the set of all closed subgroups of $G := \trm{DGal}(K/k)$ and $\mathcal{M}$ the set of all differential subfields $k \subset M \subset K$. In analogy to algebraic fields extension theory, we define 
$$\bao{rrclrcl}
\trm{Fix} : &\mathfrak{G} &\longrightarrow& \mathcal{M},& H &\longmapsto& K^H := \{x \in K : \sigma(x) = x\ \forall \sigma \in H\}\\
&&&&&\\
\trm{DGal} : & \mathcal{M} &\longrightarrow& \mathfrak{G},& M &\longmapsto& \trm{DGal}(M/k),\\
\ea$$
then we have:
\bn
\item both functors are inverse to one another.
\item $H \in \mathfrak{G}$ is normal in $G$ if and only if for $M = K^H$:
$$G(M) \subset M,$$
i.e. is $G$-invariant as a set ($g x \in M$ for all $g \in G$, $x \in M$).
\item If $H \in \mathfrak{G}$ is normal then the canonical projection $G \longrightarrow \trm{DGal}(K^H/k)$ is surjective and has $H$ as kernel. Furthermore, $K^H$ is a PV extension for some linear differential equation over $k$.
\item Let $G^0 \leq G$ be the connected component of identity, then $K^{G^0} = k$.
\en
\end{satz}
\bmk A proof is given in \cite{vdPS01}. The connected component of identity is to be understood as follows. If $(G,m,e,\tau)$ is a topological group, with $\tau$ its topology, an open subset $U \in \tau$ is called connected if the only disjoint union of open subsets $U' \cup U'' = U$ is trivial (i.e. $U' \in \{\emptyset, U\}$, $U'' = U\bsl U'$). In this case, $U$ is also called a connected component. If furthermore, $1 \in U$ is a subgroup, $U$ is called the connected component of identity which is denoted by $G^0$. These definitions were taken from \cite{Milne}.
\bsp Some examples:
\bn
\item $(\zz,+,0,\mathcal{P}(\zz))$ is a topological group wrt. discrete topology. Hence, all pointed subspaces are open subsets. Therefore, $\{0\} = \zz^0$.
\item $(\rz^\times,\cdot,1,\tau_{\trm{standard}})$ is a topological group with connected components: $\rz_{>0}, \rz_{<0}$. Hence, $\rz_{>0} = \left(\rz^{\times}\right)^0$.
\en
\subsection{Example}
Now, let us consider two simple examples.
\subsubsection{1-dim case}
Let $k = \currfield(z)$ with $\currfield$-derivation $\partial = \frac{d}{d z}$, as well as $a \neq 0$ and $\partial(x) = ax$. Here, we have a 1-dimensional differential equation, i.e. the solution space is a 1-dim $\currfield$-vector space. Let us compute the
\bd
\item[Fundamental matrix] Put $X = x \in \trm{Gl}_1(\currfield(z))$. Clearly, $Wr(X) = (x)$ and $\det Wr(X) = x \neq 0$. Hence, $x$ belongs to the $\currfield$-basis of the solution space.
\item[Picard-Vessiot ring] By prop. \ref{PicardVessiotRing} we know that $R \simeq \currfield(z)[x,1/x]$.
\item[Differential Galois group] Pick some $f \in \trm{Gl}_1(R)$, by definition $f \in \trm{DGal}(R/k)$ if $f$ commutes with $\partial$:
$$\partial(f x) = \partial(f) x + f a x = f a x = f \partial(x) \LRA \partial(f) = 0,$$
i.e. $f$ is in $\currfield.id$. Thus, we have that $\trm{DGal}(R/k) = \left<\partial\right> \simeq \qz^\times$.
\ed
\subsubsection{2-dim case} \label{twoD}
Let $k \subseteq \currfield$ with trivial derivation $\partial\mid_{\currfield} = 0_{\currfield}$, as well as $a \neq 0$ and $\partial^2(x) = a x$. Clearly, the solution space in question is contained in a two-dimensional space with companion matrix $A = \left(\bao{cc}
0 & 1\\
a & 0\\
\ea\right)$.
\paragraph{Fundamental matrix} Put $X = \left(\bao{cc}
x_{11} & x_{12}\\
x_{21} & x_{22}\\
\ea\right)$ as fundamental matrix with $\partial(x_{11}) = x_{21}, \partial(x_{21}) = a x_{11}$ as well as $\partial(x_{12}) = x_{22}, \partial(x_{22}) = a x_{12}$. The Wronskian matrix is
{\scriptsize
$$\left(\bao{cccc}
x_{11} & x_{21} & x_{12} & x_{22}\\
x_{21} & a x_{11} & x_{22} & a x_{12}\\
a x_{11} &  a x_{21} & a x_{12} & a x_{22}\\
a x_{21} & a^2 x_{11} & a x_{22} & a^2 x_{12}\\
\ea\right)
$$}
Obviously, $\det Wr(X) = 0$ as the fourth and third row are linear combinations of the first two rows. For instance, $x_1 = x_{11}$ and $x_2 = x_{21}$ forms a $\currfield$-basis of the solutions space. Since the remaining indeterminates are linearly dependent over $\currfield$ we compute for $x_{12} = \lambda_{11} x_1 + \lambda_{12} x_2, x_{22} = \lambda_{21} x_1 + \lambda_{22} x_2$ and $\lambda_{ij} \in \currfield$:
$$\bao{rclcl}
\partial(x_{12}) &=& \lambda_{11} \partial(x_1) + \lambda_{12} \partial(x_2) &=& \lambda_{12} a x_1 + \lambda_{11} x_1\\ 
&&&&\\
&\stackrel{!}{=}& \lambda_{21} x_1 + \lambda_{22} x_2 &\LRA& \lambda_{21} = \lambda_{12}, \lambda_{22} = \lambda_{11} a\\
&&&&\\
\partial(x_{22}) &=& \lambda_{21} \partial(x_1) + \lambda_{22} \partial(x_2) &=& a \lambda_{11} x_1 + a \lambda_{11} x_1\\
&&&&\\
&\stackrel{!}{=}& a \lambda_{11} x_1 + a\lambda_{12} x_2\\
\ea$$
We see that we may chose any $\lambda_{11} =: \lambda_1, \lambda_{12} = \lambda_2 \in \currfield$ such that $\det X$ is a unit. Continuing:
$$\det X = \det \left(
\bao{cc}
x_1 & \lambda_1 x_1 + \lambda_2 x_2\\
x_2 & a \lambda_2 x_1 + \lambda_1 x_2\\
\ea\right) = %a \lambda_2 x_1^2 + \lambda_1 x_1 x_2 - \lambda_1 x_1 x_2 - \lambda_2 x_2^2 
= a \lambda_2 x_1^2 - \lambda_2 x_2^2 = \lambda_2 (a x_1^2 - x_2^2).$$
We can set $\lambda_1$ arbitrarily but chose zero and $\lambda_2 = 1$,
%Identifying the remaining variables with $x_{12} = a^{-1} x_2$, $x_{22} = a x_1$
 we get the fundamental matrix $X = \left(\bao{cc}
x_1 & x_2\\
x_2 & a x_1\\
\ea\right)$. Its inverse is
$$X^{-1} = \frac{1}{a x_1^2 - x_2^2} \left(
\bao{cc}
a x_1 & -x_2\\
-x_2 & x_1\\
\ea\right)$$
In Heidereich 2010, he provides the condition $\sum_{i=0}^n a_i \partial^i(X) X^{-1} \in M_2(k)$ for the fundamental matrix. But clearly, if $\partial(X) = A X$ then $\partial^i(X) = A^i X$ and the condition holds.% In our case:
%$$\partial(X)X^{-1} = A X X^{-1} = \frac{1}{a x_1^2 - a^{-1} x_2^2}\left(\bao{cc}
%x_2 & a x_1\\
%a x_1 & a x_2\\
%\ea\right)\left(
%\bao{cc}
%x_1 & a^{-1} x_1\\
%x_2 & a x_2\\
%\ea\right) = A.$$
\paragraph{Differential Galois group} we have $g = (g_{ij}) \in \trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield)$ iff $g A - A g = 0$ and $\det g \in \currfield^\times$. We get:
$$\bao{cc}
a g_{12} - g_{21} = 0 & -a g_{12} + g_{21} = 0\\
a g_{11} - a g_{22} = 0 & -a g_{11} + a g_{22} = 0.\\
\ea$$
Therefore we have $g = \left(\bao{cc}
g_{11} & g_{12}\\
a g_{12} & g_{11}\\
\ea\right)$. The determinant is $\det g = g_{11}^2 - a g_{12}^2$. If $g_{ij} \in \currfield$ we get that $\det g = 0 \LRA g_{11}^2 = a g_{12}^2$. Since $\currfield$ is algebraically closed, there is a $b \in \currfield$ such that $b^2 = a$. Hence, $g_{11} = \pm b g_{12}$. Thus we get:
$$\left\{\left(
\bao{cc}
g_{11} & g_{12}\\
a g_{12} & g_{11}\\
\ea\right) \in \trm{Gl}_2(\currfield) : g_{11} \neq \pm b g_{12}\right\}$$
as a subgroup of the differential Galois group. To conclude, the Galois group is
$$\trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield) = \left<g\right> \supset \left<\partial\right>.$$
Now, we can express $\trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield)$ as an open algebraic set $U$ in $\aff{2}_{\currfield}$:
$$\bao{rcl}
\trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield) &\simeq_{\trm{claim}}& U\\
 &:=& \{(g_{11},g_{12}) \in \aff{2}_{\currfield} : g_{11}^2 - a g_{12}^2 \neq 0\}\\
&=& \aff{2}_{\currfield} \bsl \left\{(g_{11},g_{12}) \in \aff{2}_{\currfield} : g_{11}^2 - a g_{12}^2 = 0\right\}\\
&&\\
&=& \aff{2}_{\currfield}\bsl Z(g_{11}^2 - a g_{12}^2)\\
\ea$$
As is shown in \cite{CohCuySte}, the differential Galois group is a subgroup of $\trm{Sl}(S(L))$. Therefore we may substitute:
$$U' = \{(g_1',g_2') \in \aff{2}_{\currfield} : g_1'^2 - a g_2'^2 = 1\}.$$
The multiplication is simply:
$$m_U := \left[\left((g_{11},g_{12}) , (g'_{11},g'_{12})\right) \longmapsto (g_{11} g'_{11} + a g_{12} g'_{12}, g_{11} g'_{12} + g'_{11} g_{12})\right].$$
Checking:
$$\bao{rcl}
(g_{11} g'_{11} + a g_{12} g'_{12}, g_{11} g'_{12} + g'_{11} g_{12}) &\stackrel{?}{\in}& U:\\
&&\\
(g_{11} g'_{11} + a g_{12} g'_{12})^2 - a (g_{11} g'_{12} + g'_{11} g_{12})^2 &=& g_{11}^2 {g'}_{11}^2 + a^2 g_{12}^2 {g'}_{12}^2 + 2 a g_{11} g_{12} g'_{11} g'_{12}\\
&&- a\left(g_{11}^2 {g'}_{12}^2 + g_{12}^2 {g'}_{11}^2 + 2 g_{11} g_{12} {g'}_{11} {g'}_{12}\right)\\
&&\\
&=& (g_{11}^2 - a g_{12}) {g'}_{11} - (g_{11}^2 - a g_{12}) a {g'}_{12}\\
&&\\
&=& \underbrace{(g_{11}^2 - a g_{12})}_{= 1} \underbrace{({g'}_{11}^2 - a {g'}_{12})}_{= 1} = 1,\\
\ea$$
implying $m_U\left((g_{11},g_{12}),({g'}_{11},g'_{12})\right) \in U$. Moreover, the map $\iota: U \ni (g_{11},g_{12}) \longmapsto g \in  \trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield)$ is a morphism of groups (by definition), injective as only $(1,0) \in \ker \iota$ and surjective as $(g_{11},g_{12}) \in \iota^{-1}(g)$. This proves the claim the above defined open subset $U$ is isomorphic to $\trm{DGal}(\currfield[x_1,x_2,1/\det X]/\currfield)$.
%This group has three classes of subgroups:
%$$H_1 := \left\{\left(\bao{cc}
%g_{11} & 0\\
%0 & g_{11}\\
%\ea\right) : g_{11} \in \currfield^\times\right\},\ H_2 := \left\{\left(\bao{cc}
%0 &g_{12}\\
%a g_{12} & 0\\
%\ea\right) : g_{12} \in \currfield^\times\right\},$$
%$$\ H_3 := \left\{\left(
%\bao{cc}
%g_{11} & g_{12}\\
%a g_{12} & g_{11}\\
%\ea\right) : (g_{11},g_{12}) \in G_{<\infty} \times \currfield^\times \cap U\right\}$$
On the other hand, $G$ can be viewed as an closed subset of $\aff{3}_{\currfield}$:
$$I := \left<(X^2 - a Y^2) Z - 1\right> \subset \ov{\qz}[X,Y,Z]\ \RA\ Z(I) = \left\{\left(g_{11}, g_{12}, 1/(g_{11}^2 - a g_{12}^2)\right) : g_{11}, g_{12} \in \currfield\right\}$$
Its multiplication is given by
$$\bao{rrcl}
m_{Z(I)} :& Z(I) \times Z(I) & \longrightarrow & Z(I)\\
&&&\\
&((g_{11}, g_{12}, (g_{11}^2 - a g_{12}^2)^{-1}), & \longmapsto &(g_{11} {g'}_{11} + a g_{12} {g'}_{12}, g_{11} {g'}_{12} + g_{12} {g'}_{11},\\
& ({g'}_{11}, {g'}_{12}, ({g'}_{11}^2 - a {g'}_{12}^2)^{-1}))&&(g_{11}^2 - a g_{12}^2)^{-1}({g'}_{11}^2 - a {g'}_{12}^2)^{-1})\\
\ea$$
The proof of the above claim follows analogously to the open case. %The main difference is that $Z(I) = U \times \currfield^\times \cup \left\{0_{\aff{3}_{\currfield}}\right\}$, i.e. Zariski-dense and $Z(I)$ is a monoid (has $0$ as non-invertible and idempotent).
\paragraph{Characterization of field extension}
\bn
\item As we noted above, $\partial(\det X) = 0$. Since $R$ is a simple differential ring, we conclude $\det X \in k^\partial = k = \currfield$. Thus there is some $b = \det X \in \currfield^\times$. Therefore, $K = S^{-1}R \simeq \currfieldx[X]/\left<X^2 - a x_1^2 + b\right>$. In other words: the Picard-Vessiot field $K$ is an algebraic extension of (some appropriate) transcendental extension of $\currfield$. Rescaling our differential equation, we may assume that $b = 1$.
\item An immediate consequence of the above is the following: We consider $R' := \currfield[x_1,x_2] \simeq \trm{Sym}(S(L))$ wrt. the $\currfield$-basis $\{x_i : i = 1,2\}$. Now, take $I = \left<x_2^2 - a x_1^2 + 1\right> \subset \trm{Sym}(S(L))$, we get:
$$\trm{Sym}(S(L))/I \simeq R'/\left<x_2^2 - a x_1^2 + 1\right>.$$
Since the $\currfield$-derivation does not reduce the degree of any polynomial of positive degree we can use the degree induced filtration on $\trm{Sym}$:
$$\filteredA^{\leq l} = \left\{x \in \trm{Sym}(S(L)) : \deg x \leq l\right\} \ \RA \partial_{\trm{Sym}}\mid_{\filteredA^{\leq l}} (\filteredA^{\leq l}) \subset \filteredA^{\leq l},\ \filteredA^{\leq 0} := \currfield,\ \filteredA^{\leq -n} := \{0\}, n, l \in \nz,$$
i.e. $\filteredA^{\leq l}$ is $\partial_{\trm{Sym}}$ invariant $\currfield$-subspace. Here we set $\deg(0) := -\infty$ and $\deg$ is the sum degree. On the other hand:
$$\filteredA^{\leq 2} = \bigoplus_{|\alpha| \leq 2} \currfield.x_1^{\alpha_1} x_2^{\alpha_2}\ \RA\ \filteredA^{\leq2} / \left(\filteredA^{\leq2} \cap I\right) = \currfield \oplus \currfield.x_1 \oplus \currfield.x_2 \oplus \currfield.x_1^2 \oplus \currfield.x_1 x_2,$$
in particular $\lambda_{0,2} x_2^2 = \lambda_{0,2} - a \lambda_{0,2} x_1^2 \in \currfield \oplus \currfield.x_1^2$ for any $\lambda_{0,2} \in \currfield$. In general, for $\filteredA^{\leq n}$ we may substitute all terms $x_2^{2 l}$ with $(a x_1^2 - 1)^l$ and all $x_2^{2l + 1}$ with $(a x_1^2 - 1)^l x_2$, for all $l \in \{d \in \nz : 2 \mid d\}$. Therefore the quotient spaces are
$$A_n := \filteredA^{\leq n}/ \left(\filteredA^{\leq n} \cap I\right) /\left(\filteredA^{\leq n - 1} / \left(\filteredA^{\leq n - 1} \cap I\right) \right)\simeq \currfield.x_1^n \oplus \currfield.x_1^{n-1} x_2,$$
and therefore its associated graded $\currfield$-algebra is obviously:
$$A = \bigoplus_{n \geq 0} A_n \simeq \currfield[x_1] \oplus \currfield[x_1].x_2.$$
Summarizing we get
$$\trm{Sym}(S(L))/I = \filteredA/I = \bigcup_{n \geq 0} \filteredA^{\leq n}/  \filteredA^{\leq n} \cap I \simeq A \simeq \currfield[x_1] \oplus \currfield[x_1].x_2,$$
with multiplication
$$\mu = \left[x_1^{i_1} x_2^{j_1} \otimes x_1^{i_2} x_2^{j_2} \longmapsto x_1^{i_1 + i_2} (a x_1^2 - 1)^{(j_1 + j_2 - (j_1 + j_2 \mod 2))/2} x_2^{j_1 + j_2 \mod 2}\right],\ \forall i_l \in \nz, j_l = 0, 1.$$
We note that the subalgebra $\currfield[x_1]$ is not a differential algebra as 
$$\partial = \left[x_1^i x_2^j \longmapsto \begin{cases}
(1 + i) a x_1^{i+1} - i x_1^{i-1}& j = 1\\
i x_1^{i-1} x_2 & j = 0\\
\end{cases}\right].$$
Localizing wrt. $S := \currfield[x_1]\bsl\{0\}$, i.e. non-zero polynomials, and identifying with all corresponding symmetric tensors we have that
$$K^+ := S^{-1} \left(\currfield[x_1] \oplus \currfield[x_1].x_2\right) = S^{-1} \currfield[x_1] \oplus S^{-1} \currfield[x_1].x_2 = \currfieldx \oplus \currfieldx.x_2.$$
However, we have not shown yet whether $K^+$ is an integral domain. Now let us consider $p = Y^2 - a x_1^2 + 1 \in \currfieldx[Y]$. 
$$Y^2 - \underbrace{a x_1^2 + 1}_{= \alpha^2} = (Y - \alpha)(Y + \alpha).$$
If $\alpha \in \currfieldx$, then $\ov{Y \pm \alpha} = \ov{Y} \pm \alpha$ are zero-divisors in the quotient ring and with $\trm{gcd}(Y - \alpha, Y + \alpha) = 1$ we get
$$\currfieldx[Y]/\left<Y^2 - a x_1^2 + 1\right> \simeq \currfieldx^2$$
as a $\currfieldx$-vector space, with multiplication:
$$\mu(a \otimes \_) := [b \longmapsto a b] \equiv \left(\bao{cc}
a_0 & (a x_1^2 - 1) a_1\\
a_1 & a_0\\
\ea\right) \in M_2(\currfieldx),\ \forall a \equiv (a_0, a_1) \in \currfieldx^2$$
and with derivation:
$$\partial = \left[
\bao{ccc}
(1_{\currfield} x_1^i, 0) &\longmapsto& (0 , i x_1^{i-1})\\
(0, 1_{\currfield} x_1^i) &\longmapsto& ((1 + i) a x_1^i - i x_1^{i-1},0)\\ 
\ea
\right].$$ Hence, the Picard-Vessiot field $K$ would be simply $\currfieldx$. This implies:
$$\exists \alpha \in \currfield.x_1 \bsl \{0\}:\ \partial_{\currfieldx}(\alpha) \in \currfield.x_1 \LRA \exists v \in \ker (\alpha id_{\currfield.x_1} - \partial) \bsl\{0\},$$
i.e. an eigenvector of $\partial_K$. On the other hand, is $[\currfield(x_1,\alpha) : \currfieldx] := \deg \trm{Min}(\alpha,\currfieldx) = 2$:
$$\RA\ (K, \partial_K) \simeq \left(\currfield\left(x_1,\sqrt{a x_1^2 - 1}\right), \partial_{K_1}\right)$$% \simeq \left(\currfield\left(x_2,\sqrt{(1 + x_2^2)/(a)}\right), \partial_{K_2}\right),$$
is a field, with $\currfieldx/\currfield$ transcendental and with $\currfield$-derivation:%s:
$$\bao{rrcl}
\partial_{K}%_1}
 : &\currfield\left(x_1,\sqrt{a x_1^2 - 1}\right)&\longrightarrow&\currfield\left(x_1,\sqrt{a x_1^2 - 1}\right)\\
&&&\\
&x_1 &\longmapsto& \sqrt{a x_1^2 - 1}\\
&\sqrt{a x_1^2 - 1} &\longmapsto& a x_1\\
&&&\\
%\partial_{K_2} : &\currfield\left(x_2,\sqrt{(1 + x_2^2)/a}\right)&\longrightarrow&\currfield\left(x_2,\sqrt{(1 + x_2^2)/a}\right)\\
%&&&\\
%&x_2 &\longmapsto& \sqrt{\frac{1 + x_2^2}{a}}\\
%&\sqrt{\frac{1 + x_2^2}{a}} &\longmapsto& a x_2\\
\ea$$
However, we claim that since $x_1$ is transcendental over $\currfield$ and the polynomial $a x_1^2 +1$ is no square in $\currfield[x_1]$, the polynomial $X^2 - a x_1^2 + 1$ is irreducible over $\currfieldx$. First, let us assume there is a root $\alpha = \frac{\beta}{\gamma} \in \currfieldx$, where $\trm{gcd}(\beta,\gamma) = 1$ and $\gamma \neq 0$. Then we compute:
$$\alpha^2 = a x_1^2 - 1 = \frac{\beta^2}{\gamma^2} \LRA \beta^2 = \gamma^2 (a x_1^2 - 1),$$
i.e. $\beta \in \currfield[x_1]$ is a root of the polynomial $X^2 - \gamma^2(a x_1^2 - 1)$. Having $\beta = \sum_{j=0}^{n_1} \beta_j x_1^i, \gamma = \sum_{i=0}^{n_2} \gamma_i x_1^i$, with $\gamma_i, \beta_j \in \currfield$, we get that $n_1 = n_2 + 1$ due to the factor $a x_1^2$. On the other hand, both decompose to linear factors due to algebraic closedness of $\currfield$, e.g.:
$$\beta = \wt{\beta}_0 \prod_{i=1}^{m_1} (x_1 - \wt{\beta}_i)^{s_i},\ \gamma = \wt{\gamma}_0 \prod_{j=1}^{m_2} (x_1 - \wt{\gamma}_j)^{t_j},\ \trm{and}\ \sum_{i=0}^{m_1} s_1 = \sum_{j=0}^{m_2} t_j + 1.$$
Clearly, all but at most two factors on both sides of our polynomial equation cancel. So we get $n_2 \leq 0$ and $n_1 \leq 1$. In particular, $\alpha$ itself is already in $\currfield[x_1]$ for $n_2 = 0$ (i.e. $\gamma \neq 0$). This is not possible, as
$$\wt{\beta}_0^2 (x_1 - \wt{\beta}_1)^2 = \wt{\beta}_0^2(x_1^2 - 2 \wt{\beta}_1 x_1 + \wt{\beta}_1^2) = \wt{\gamma}_0^2 (a x_1^2 - 1) \LRA 2 \wt{\beta}_1 = 0 \wedge \wt{\beta}_0^2 \wt{\beta}_1^2 = -\wt{\gamma}_0^2$$
leads to the contradiction $\gamma = 0$. Hence, $\alpha$ is algebraic and $[\currfieldx(\alpha) : \currfieldx] = 2$.\\
\indent Nevertheless, one aspect we have not considered yet which we will discuss now.
%However, since $\currfield$ is algebraically closed, we get the abovementioned decomposition which will get discussed right now.
\item $\currfield(x_1,x_2) = K$ is contained in a $\currfield$-algebra generated by one distinct transcendental element $y$ being a unit in $\currfield[x_1,x_2]$: The ring $R = \currfield[x_1,x_2,\det X^{-1}]$ contains four non-trivial units (i.e. elements in $R^\times \bsl \currfield^\times$). Namely:
$$\pm \sqrt{a} x_1 \pm x_2,$$
to specify: four non-trivial divisors of $\det X$ in $\currfield[x_1,x_2]$. Computing their derivative wrt. $\partial$, we get:
$$\bao{rclcl}
\partial(\sqrt{a} x_1 \pm x_2) &=& \sqrt{a} x_2 \pm a x_1 &=& \pm \sqrt{a} (\sqrt{a} x_1 \pm x_2)\\
&&&&\\
\partial(-\sqrt{a} x_1 \pm x_2) &=& -\sqrt{a} x_2 \pm a x_1 &=& \mp\sqrt{a} (-\sqrt{a} x_1 \pm x_2).\\
\ea$$
Each of the four elements is an eigenvector of $\partial$ in $R$ with eigenvalue $\pm \sqrt{a}$ - or equivalently, solves the following differential equations:
$$L_{\pm} := \partial(x) \mp \sqrt{a} x = 0,$$
where the subscript sign defines the sign of the eigenvalue. Defining $y_1 \in S(L_+), y_{-1} \in S(L_-)$ we get that $K = \currfield(x_1,x_2)$ contains two subfields depicted in the following diagram:
$$\xymatrix{
& \currfield(y_1) \ar@{^{(}->}[rd]&\\
\currfield\ar@{^{(}->}[rd]\ar@{^{(}->}[ru]&&\currfield(x_1,x_2) \simeq \currfield(y_1)\\
& \currfield(y_{-1}) \ar@{^{(}->}[ru]&\\
}$$
By direct computation, we see that:
$$L = \partial^2 - a \cdot id = (\partial - \sqrt{a}) (\partial + \sqrt{a}).$$
And our differential module $S(L)$ decomposes:
$$S(L)^* \simeq \currfield[\partial]/\currfield[\partial].L \simeq \currfield[\partial]/\currfield[\partial].L_+ \oplus \currfield[\partial]/\currfield[\partial].L_- \simeq S(L_+)^* \oplus S(L_-)^*.$$
\en
\paragraph{The subfields}
Firstly, we like to compute the Picard-Vessiot ring for $M_{i} = \currfield(y_i)$:
$$R_i = \currfield[y_{i}, y_i^{-1}],\ \trm{for}\ i = \pm1$$
%As the image of the ideal $\left<y_i\right> \subset \currfield[y_i]$ is the whole ring under inclusion or equivalently $1_R \in R_i.y_i$, we have indeed that $R_i$ is simple differential. 
%Let $I \subset R_i$ be a proper ideal and we assume differential closedness - i.e. $\partial(I) \subset I$.
%As a noetherian $R$-submodule of a noetherian module $R$ (generated by $y_{\pm 1}$ over $\currfield$), $I$ is finitely generated. Hence, let $S:= \{s\} \subset I$ be one generating set. By differential closedness, we get for any $s \in S$:
%$$\partial(s) = \partial\left(\sum_{i=-m}^n s_i y_1^i\right) = \sum_{i=-m}^n s_i \partial(y_1^i) = \sum_{i=-m}^n i \sqrt{a} s_i y_1^i \in I$$
%$$\LRA \partial(s) - s = \sum_{i=-m}^n (i \sqrt{a} - 1) s_i y_1^i \in I$$
%But both, $s, \partial(s) - s$ are of degree $n$, or $m$ wrt. $y_{\pm 1}$ and $y_1^m (\partial(s) - s) \in \currfield[y_1]$.
% There is an ideal $I' \subset \currfield[y_1]$, such that $S_{y_1}^{-1}(I') \supset I$. By definition of $I$, we get
%$$y_1^m t \in I' \RA \partial(y_1^m t) = \underbrace{m \sqrt{a} y_1^m t}_{\in I'} + \underbrace{y_1^m \partial(t)}_{\in \partial(I')},$$
%but identifying $I' := I \cap \frac{\currfield[y_1]}{1}$ we get $\partial(I') \subset I'$. Being a PID, all ideals $I' \subset \currfield[y_1]$ are of the form $\left<s\right>$. On the other hand, $\partial$ operates on all weight spaces $\currfield.y_1^i$, $i \geq 1$, invariantly:
%$$\bao{rrcl}
%\partial_i := \partial\mid_{\currfield.y_1^i} : &\currfield.y_1^i &\longrightarrow& \currfield.y_1^i\\
%&&&\\
%&y_1^i &\longmapsto&i \sqrt{a} y_1^i\\
%\ea$$
%Thus we have:
%$$\partial s = \sum_{i=0}^n s_i \partial(y_1^i) = \sum_{i=0}^n i \sqrt{a} s_i y_1^i \in \left<s\right> \LRA \partial(s) \equiv 0 \mod s$$
%$$\LRA \sum_{i=0}^{n-1} (i - n) \sqrt{a} s_i y_1^i = 0 \LRA s_i = 0 \vee n - i = 0\ \forall 0 \leq i \leq n - 1,$$
%Hence, each derivative of the generators $s$ agree in degree but also reduce to zero modulo $\left<s\right>$ contradicting our claim $\partial(s) \in \left<s\right>$. Thus, all $D$-stable ideals in $R$ are indeed trivial.
Firstly, we get:
$$\partial(y_i^{-1}) = -\frac{\partial(y_i)}{y_i^2} = -i \sqrt{a} y_i^{-1}, i =\pm1$$
implying the multiplicative inverse solves the opposite differential equation. Hence, $R_i$ already contains all solutions of $L_\pm$ by simply putting $y_{-1} := y_1^{-1}$.\\
\indent Let $R = R_1$. Furthermore, we want to show that $R$ is indeed a simple differential ring over $\currfield$. Clearly, $\partial$ operates invariantly on $\currfield.y_{\pm 1}^i$ for all $i \neq 0$. In addition, $R$ is isomorphic to $S_{X}^{-1}\currfield[X]$, the localization of $\currfield[X]$ where
$$S_X = \currfield[X] \bsl \bigcup_{\substack{\mathfrak{p} \in \trm{Spec}(\currfield[X])\\X \notin \mathfrak{p}}} \mathfrak{p} = \{f \in \currfield[X] : f \notin \idealp\ \forall \idealp \in \trm{Spec} \currfield[X] \bsl \{\left<X\right>\}\}.$$ Furthermore, every prime ideal in $\currfield[X]$ is generated by $X - \alpha$, for some $\alpha \in \currfield$ as $\currfield$ is algebraically closed. Hence, on the one hand we get:
$$S_X = \{f \in \currfield[X]\bsl \currfield : f \notin \left<X - \alpha\right> \forall \alpha \in \currfield^\times\} = \{X^i : i \geq 0\}$$
and on the other hand: if $I' \subset R$ is an ideal, there is an ideal  $I \subset \currfield[X]$ such that $I' = S_{X}^{-1} I$. This implies every ideal is principal in $R$. In $\currfield[X]$ each element $\mathfrak{p} \in \trm{Spec}(\currfield[X])$ gets mapped to $\left<X\right>$ via $\partial$, as every element $p (X - \alpha)$ in a prime ideal $\left<X - \alpha\right>$ has an image in $\left<X\right>$:
$$p (X - \alpha) \longmapsto \partial(p) (X - \alpha) + \sqrt{a} p X,\ \forall \alpha \in \currfield,\ p \in \currfield[X]$$
where the constant coefficient $\left(\partial(p)\right)_0 = 0 \LRA p_0 \in \ker \partial \LRA \partial(p) \in \left<X\right>$. This shows the differential $\currfield$-algebra $\left(\currfield[X], \partial = \sqrt{a} X \cdot \frac{d}{d X}\right)$ has only one differential prime ideal, $\left<X\right>$ (- in our case even maximal). However, the localization cancels this as its generator $X$ is a unit in $R$ (using variable notation $X, y_1$ interchangeably). Therefore, our rings $R_1, R_{-1}$ are indeed simple differential rings - or PV. In words of $D$-module algebra, $R_1, R_{-1}$ are simple $D$-rings, for $D = \currfield[\partial]$.\\
\indent To conclude this paragraph, we want to show that $\currfield(x_1,x_2) \simeq \currfield(y_1)$ as differential field extensions over $\currfield$. To achieve this we need to show that the maps:
$$\bao{rrcl}
\Phi : & \currfield(y_1) &\longrightarrow& \currfield(x_1,x_2)\\
& y_1 &\longmapsto & \sqrt{a} x_1 + x_2\\
& y_1^{-1}&\longmapsto & \sqrt{a} x_1 - x_2\\
&&&\\
\Psi : &\currfield(x_1,x_2) &\longrightarrow& \currfield(y_1)\\
& x_1 &\longmapsto& \frac{1}{2 \sqrt{a}} (y_1 + y_1^{-1})\\
& x_2 &\longmapsto& \frac{1}{2} (y_1 - y_1^{-1})\\
\ea$$\label{PVisomorph}
are bijective and inverse to one another as differential $\currfield$-algebra homomorphisms (or $D$-module algebra homomorphisms). Clearly, the two $\currfield$-vector spaces $V_1 := \currfield.y_1 \oplus \currfield.y_1^{-1}$ and $V_2 := \currfield.x_1 \oplus \currfield.x_2$ are isomorphic by simple basis change induced by the restrictions $\mid_{V_i}$ of the above $\currfield$-vector space homomorphisms. Furthermore, $\Psi \mid_{V_2} \Phi \mid_{V_1} = id_{V_1}$ and $\Phi \mid_{V_1} \Psi \mid_{V_2} = id_{V_2}$ and both are $D$-modules. Hence, the basis change respects this property as shown above. This property is kept for the recursively defined family of $D$-modules:
$$V_{1,i} := V_{1,i-1} \oplus \currfield.y_1^i \oplus \currfield.y_1^{-i},\ V_{2,i} := V_{2,i-1} \oplus \currfield.x_1^i \oplus \currfield.x_1^{i-1} x_2, \forall i \geq 1$$
and $V_{j,0} = \currfield$ for $j = 1, 2$. We get:
$$\bao{rclclcl}
\bigcup_{i \geq 0} V_{1, i} &\simeq& \currfield[y_1,X]/\left<y_1 X - 1\right> &\simeq& \currfield[y_1,y_1^{-1}] &=:& R_1\\
&&&&\\
\bigcup_{i \geq 0} V_{2,i} &\simeq& \currfield[x_1,x_2]/\left<x_2^2 - a x_1^2 + 1\right> &\simeq& \currfield[x_1,\sqrt{a x_1^2 - 1}] &=:& R_2,\\
\ea$$
i.e. a $D$-stable filtration for each subalgebras $R_1, R_2$. Having already shown their simplicity we only need surjectivity as $\Psi\mid_{V_{2,0}} = id_{V_{2,0}}, \Phi\mid_{V_{1,0}} = id_{V_{1,0}}$ already implies injectivity. Let $f$ and $g$ equal $\sum_i \left(f_{1,i} y_1^i + f_{-1,i} y_1^{-i}\right) \in R_1$ and $\sum_i \left(g_{1,i} x_1^i + g_{2,i-1} x_1^i x_2\right) \in R_2$, respectively. Obviously, $\sum \left(f_{1,i} (\sqrt{a} x_1 + x_2)^i + f_{-1,i} (\sqrt{a} x_1 - x_2)^i\right)$ and
$\sum (y_1 + y_1^{-1})^i \left(\frac{g_{1,i}}{2 \sqrt{a}} + \frac{g_{2,i}}{2} (y_1 - y_1^{-1})\right)$ are elements in their respective preimages. This extends naturally to their localizations.
%Hence, any ideal stable under $\partial$ is, on the one hand, generated by a single element, on the other hand, $\deg f = \deg \partial(f)$, where $\deg = \left[f = \sum_{i=-m}^n f_i X^i \longmapsto \max(m,n)\right]$. In particular, $\left<f, \partial f\right>$ is also principal and a subideal of $I$.
\paragraph{Galois group of the subfields}
As above $\trm{DGal}(\currfield[y_1,y_{-1}]/\currfield) = \{a \in \trm{Gl}_1(\currfield) : \partial a = a \partial\}$. Obviously, the unit group of $\currfield$ is our differential Galois group.
% However, both generators are eigenvectors. Hence, the diagonal matrix
%$$A_{L_++L_-} = \left(\bao{cc}
%\sqrt{a} & 0\\
%0 & -\sqrt{a}\\
%\ea\right)$$
%represents $\partial$ on $S(L_+) \oplus S(L_-)$ with canonical basis vectors identified with $y_1, y_{-1}$, respectively. Clearly, all matrices with diagonal entries are the only ones fulfilling our definition. We get
%$$\trm{DGal}(\currfield[y_1,y_{-1}]/\currfield) \simeq \currfield^\times \times \currfield^\times.$$
\paragraph{Conclusion}
Given our above example, the PV-ring and field are
$$R = \currfield[y_1,y_{-1} := y_1^{-1}],\ K = S^{-1}R = \currfield(y_1),$$
respectively, with differential Galois group
$$\trm{DGal}(\currfield[y_1,y_{-1}]/\currfield) \simeq \currfield^\times.$$
The differential ring $\currfield[x_1,x_2] = \trm{Sym}(L)/\left<x_2^2 - a x_1^2 + 1\right> \simeq \trm{Sym}(\currfield^2)/\left<e_2^2 - a e_1^2 + 1\right>$ is isomorphic to
$$\currfield[y_1] \oplus \currfield[y_{-1}] \simeq \currfield[y_1]^2.$$
\bmk %The results are valid in case $a, \sqrt{a} \in \currfield[z]\bsl\currfield$, if our matrix equation gets slightly modified:
%$$A_L = \left(\bao{cc}
%0 & 1\\
%a \pm \partial(\sqrt{a}) & 0\\
%\ea\right) \in \trm{Gl}_2(\currfield(z))\LRA L(y) = \partial^2(y) - \left(a \mp \partial(\sqrt{a})\right)y = 0,\ \forall y \in S(L).$$
%The different signs arise from two isomorphic solution spaces $S_1 = S(L_+) \oplus S(L_-)$, $S_2 = S(L_-) \oplus S(L_+)$, or equivalently, by the order of operator evaluation:
%$$S_1 \simeq D/D.(\partial - \sqrt{a})(\partial + \sqrt{a}),\ S_2 \simeq D/D.(\partial + \sqrt{a})(\partial - \sqrt{a}),\ \trm{where}\ D := \currfield(z)[\partial], \partial(z) = 1.$$
A more exhaustive approach to $\partial^2 - a \cdot id$, where $a$ is not a constant, is given in \cite{vdPS01} and \cite{CohCuySte}.\\
\indent Unfortunately, our given examples are all linear homogeneous ODEs. The theory, we are going to present in the next chapter, is more general. It includes theory for general ODEs (linear, non-linear) in characteristic zero, (multivariate) iterative derivations in positive characteristic and difference equations in arbitrary characteristic.\\
\indent Nevertheless, as the general theory does not require the ring/field of constants to be algebraically closed we may examine a broader setting.
%though each factor is not differentially closed (i.e. $(1,0) \stackrel{\partial}{\longmapsto} (0,1) \stackrel{\partial}{\longmapsto} (a,0)$).
%We want to see if the following $\currfield$-linear maps are differentially invariant:
%$$\sigma_1 = [\pm y_1 \longmapsto \mp y_1,\ \pm y_1^{-1} \longmapsto \mp y_1^{-1}] \in \trm{Gl}(R_1)$$
%$$\sigma_2 = [y_1 \longmapsto y_1^{-1}] \in \trm{Gl}(R_1)$$
%Since $\pm y_1$ and $\pm y_1^{-1}$ have the same eigenvalue $\sqrt{a}$ and $-\sqrt{a}$ respectively, we have that $\sigma_1$ commutes with $\partial$. Hence, $\sigma_1 \in \trm{DGal}(M_1/\currfield)$. On the other hand, $\sigma_2(\partial(y_1)) = \sigma_2(\sqrt{a} y_1) = \sqrt{a} y_1^{-1} \neq \partial(\sigma_2(y_1)) = \partial(y_1^{-1}) = -\sqrt{a} y_1^{-1}$. %Now, we may express $v = \sum_{i \in \{1,2\}} v_i x_i \in \trm{Sol}(\partial^2 - a \cdot id)$ as linear combinations of $v = \sum_{i \in \{\pm 1\}} v'_i y_i$:
%$$\bao{rrcl}
%M_{B_x}^{B_y}(id):& v &=& v'_1 y_1 + v'_{-1} y_{-1}\\
%& &=& v'_1 (\sqrt{a} x_1 + x_2) + v'_{-1} (\sqrt{a} x_1 - x_2)\\
%&&&\\
%&&=& \sqrt{a} (v'_1 + v'_{-1}) x_1 + (v'_1 - v'_{-1}) x_2\\
%&&&\\
%M_{B_y}^{B_x}(id)&v &=& v_1 x_1 + v_{2} x_{2}\\
%&&=& \frac{v_1}{2 \sqrt{a}}(y_1 + y_{-1}) + \frac{v_{2}}{2} (y_1 - y_{-1})\\
%&&&\\
%&&=& \frac{1}{2 \sqrt{a}} \left[(v_1 + \sqrt{a} v_2) y_1 + (v_1 - \sqrt{a} v_2) y_{-1}\right]\\
%\ea$$
%Hence, we have the following basis transformation matrices:
%$$M_{B_x}^{B_y} = \left(\bao{cc}
%\sqrt{a} & \sqrt{a}\\
%1 & -1\\
%\ea\right),\ \ M_{B_y}^{B_x} = \frac{1}{2 \sqrt{a}}\left(\bao{cc}
%1 & \sqrt{a}\\
%1 & -\sqrt{a}\\
%\ea\right)$$
%The conjugate $M_{B_y}^{B_x} G M_{B_x}^{B_y}$ of $G:= \trm{DGal}(\currfield(x_1,x_2)/\currfield)$ defines a group action on $S(L_+) \oplus S(L_-)$ or equivalently, makes $S(L_+) \oplus S(L_-)$ a $G$-module:
%$$\bao{rrcl}
%\alpha : &G \times \left(S(L_+) \oplus S(L_-)\right)& \longrightarrow &S(L_+) \oplus S(L_-)\\
%&&&\\
%&\left(g := \left(\bao{cc}
%g_{11} & g_{12}\\
%a g_{12} & g_{11}\\
%\ea\right), v'_1 y_1 + v'_{-1} y_{-1}\right) &\longmapsto&M_{B_y}^{B_x} g %\left(\bao{cc}
%g_{11} & g_{12}\\
%a g_{12} & g_{11}\\
%\ea\right)
%M_{B_x}^{B_y}\left(\bao{c}
%v'_1\\
%v'_{-1}\\
%\ea\right)\\
%\ea$${\footnotesize
%$$M_{B_y}^{B_x}g %\left(\bao{cc}
%g_{11} & g_{12}\\
%a g_{12} & g_{11}\\
%\ea\right)
%M_{B_x}^{B_y}\left(\bao{c}
%v'_1\\
%v'_{-1}\\
%\ea\right) = \frac{1}{2 \sqrt{a}}\left(\bao{cc}
%2 \sqrt{a} g_{11} + (1 + a^2) g_{12}& -(1 - a^2) g_{12}\\
%(1 - a^2) g_{12} & 2 \sqrt{a} g_{11} - (1 + a^2) g_{12}\\
%\ea\right) \left(\bao{c}
%v'_1\\
%v'_{-1}\\
%\ea\right)\\
%$$}
%The fixed field $\currfield(y_1)^G$ is 
%Multiplying $T$ with the generator of $\trm{DGal}(\currfield(x_1,x_2)/\currfield)$ we get:
%$$T g = \left(\bao{cc}
%\sqrt{a} & 1\\
%\sqrt{a} & -1\\
%\ea\right) \left(\bao{cc}
%g_{11} & g_{12}\\
%a g_{12} & g_{11}\\
%\ea\right) = \left(\bao{cc}
%\sqrt{a} g_{11} + a g_{12} & \sqrt{a} g_{12} + g_{11}\\
%\sqrt{a} g_{11} - a g_{12} & \sqrt{a} g_{12} - g_{11}\\
%\ea\right)$$
%Analogously to the classical Galois theory, there exists subgroups $H$ in $\trm{DGal}(\currfield(x_1,x_2)/\currfield)$ such that $\currfield(x_1,x_2)^H := \{x \in K : \sigma(x) = x \forall \sigma \in H\}$ is a Picard-Vessiot field over $\currfield$. Conversely, the $\trm{DGal}(\currfield(x_1,x_2)/M)$ is a (closed) subgroup of $\trm{DGal}(\currfield(x_1,x_2)/\currfield)$ for any intermediate Picard-Vessiot field $\currfield \subset M \subset \currfield(x_1,x_2)$. %Since we already have two Picard-Vessiot subfields $M_i = \currfield(y_i)$ of $K$, we only need to compute the Galois groups of both differential fields.